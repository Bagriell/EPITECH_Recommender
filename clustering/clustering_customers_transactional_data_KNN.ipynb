{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muQq5OB0gKEc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd                                                 # data processing    \n",
        "from tqdm import tqdm                                               # progress bar\n",
        "from sklearn.preprocessing import OneHotEncoder                     # features normalization\n",
        "import numpy as np\n",
        "\n",
        "from kmodes.kprototypes import KPrototypes\n",
        "from sklearn.cluster import KMeans                                  # clustering\n",
        "from sklearn.cluster import MiniBatchKMeans                         # clustering low-cost\n",
        "from sklearn.metrics import silhouette_score as ss \n",
        "from sklearn.metrics import silhouette_samples                      # model config\n",
        "from yellowbrick.cluster import SilhouetteVisualizer                # model metrics viz\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler      # features normalization\n",
        "from sklearn.compose import ColumnTransformer                       # features normlaization\n",
        "from sklearn.decomposition import PCA                               # dimension reduction\n",
        "\n",
        "import matplotlib.pyplot as plt                                     # data viz\n",
        "import seaborn as sns                                               # data viz    \n",
        "from yellowbrick.cluster import KElbowVisualizer                    # model config\n",
        "import pandasql as ps\n",
        "import pickle\n",
        "import pprint\n",
        "from uuid import uuid4\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = uuid4()\n",
        "key"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "notebook configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    # nb found by Elbow technique\n",
        "    \"n_clusters\": 6,\n",
        "    # repro\n",
        "    \"random_state\":29,\n",
        "    # max iteration per run\n",
        "    \"max_iter\": 200,\n",
        "    # run number\n",
        "    \"n_init\": 10,\n",
        "    \n",
        "    # pca config - level of variance keeped by algorithm\n",
        "    \"n_components\": 2,\n",
        "    # save both model and pca_model\n",
        "    \"save_model\": True,\n",
        "    # load sample data\n",
        "    \"sample\": True,\n",
        "    # run KElbowVisualizer\n",
        "    \"find_k\": False,\n",
        "    # exploratory plots\n",
        "    \"plot\": True,\n",
        "    # run id\n",
        "    \"run_id\": key\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wZzFnN-vCK1p"
      },
      "source": [
        "get data - full csv or 0.05% sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2pgFjMJhhDN"
      },
      "outputs": [],
      "source": [
        "data_fn = \"sample_KaDo.csv\" if config.get(\"sample\") else \"Kado.csv\"\n",
        "df = pd.read_csv(f\"../data/{data_fn}\")\n",
        "df.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data cleaning\n",
        "\n",
        "- check presence of na/null values\n",
        "- get rid of \"FAMILY\" underrepresented values\n",
        "- replace product prices by median values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask = ~df[\"FAMILLE\"].isin([\"MULTI FAMILLES\",\"SANTE NATURELLE\"])\n",
        "df = df[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config.get(\"plot\"):\n",
        "    df['PRIX_NET'].nlargest(50).plot(kind=\"bar\")\n",
        "    plt.xlabel('Product')\n",
        "    plt.ylabel('Price')\n",
        "    plt.title('50 most expensive products')\n",
        "    plt.tick_params(\n",
        "        axis='x',\n",
        "        which='both',   \n",
        "        bottom=False,   \n",
        "        labelbottom=False)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config.get(\"plot\"):\n",
        "    df['PRIX_NET'].nsmallest(50).plot(kind=\"bar\")\n",
        "    plt.xlabel('Product')\n",
        "    plt.ylabel('Price')\n",
        "    plt.title('50 cheapest products')\n",
        "    plt.tick_params(\n",
        "        axis='x',          \n",
        "        which='both',      \n",
        "        bottom=False,     \n",
        "        labelbottom=False)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['PRIX_NET'] = df.groupby('LIBELLE')['PRIX_NET'].transform('median')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if config.get(\"plot\"):\n",
        "#     df['PRIX_NET'].nlargest(50).plot(kind=\"bar\")\n",
        "#     plt.xlabel('Product')\n",
        "#     plt.ylabel('Median prices')\n",
        "#     plt.title('50 most expensive products with median prices')\n",
        "#     plt.tick_params(\n",
        "#         axis='x',          \n",
        "#         which='both',     \n",
        "#         bottom=False,     \n",
        "#         labelbottom=False)\n",
        "\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if config.get(\"plot\"):\n",
        "#     df['PRIX_NET'].nsmallest(50).plot(kind=\"bar\")\n",
        "#     plt.xlabel('Product')\n",
        "#     plt.ylabel('Median prices')\n",
        "#     plt.title('50 cheapest products with median prices')\n",
        "#     plt.tick_params(\n",
        "#         axis='x',\n",
        "#         which='both',\n",
        "#         bottom=False,\n",
        "#         labelbottom=False)\n",
        "\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1USRVR6jdBj"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"\n",
        "Number of client: {'{:,}'.format(df.CLI_ID.nunique())}\n",
        "Number of transactions: { '{:,}'.format(df.TICKET_ID.nunique())}\n",
        "Number of product: {len(df)}\n",
        "Shape: { '{:,}'.format(df.shape[0])} - {df.shape[1]}\n",
        "\"\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "exynVSRKNNvY"
      },
      "source": [
        "## data extraction\n",
        "\n",
        "Features are extracted from dataset to build the model. \n",
        "We are using the % of money spent by each customer by family, since this is the group with very low cardinality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group the data by CLI_ID and FAMILLE\n",
        "df_grouped = df.groupby(['CLI_ID', 'FAMILLE'])\n",
        "\n",
        "# Calculate the total sales for each group\n",
        "df_sales = df_grouped['PRIX_NET'].sum().reset_index()\n",
        "\n",
        "# Pivot the data to get the sales for each FAMILLE for each CLI_ID\n",
        "df_pivot = df_sales.pivot(index='CLI_ID', columns='FAMILLE', values='PRIX_NET')\n",
        "\n",
        "# Calculate the percentage of sales for each FAMILLE for each CLI_ID\n",
        "df_pct = df_pivot.div(df_pivot.sum(axis=1), axis=0).mul(100)\n",
        "\n",
        "# fill na value with zero\n",
        "df_pct.fillna(0, inplace=True)\n",
        "\n",
        "# rename columns\n",
        "df_pct.columns = [f\"% {col_name}\" for col_name in  df_pct.columns]\n",
        "df_pct"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bquaicSwL5rW"
      },
      "source": [
        "## normalization\n",
        "\n",
        "no need since we have all value on same scale (%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iJKsD2PJZAB"
      },
      "outputs": [],
      "source": [
        "df_features = df_pct.copy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn9OM6GtT-6k"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "The idea is to find the right number of cluster in our dataset.  \n",
        "Then train the clustering model and analyze it.  \n",
        "We'll use the silhouette score as metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KElbowVisualizer package allow us to find optimal number of clusters\n",
        "\n",
        "if config.get('find_k'):\n",
        "    model = MiniBatchKMeans(\n",
        "        n_init=config.get(\"n_init\") or 10,\n",
        "        random_state=config.get(\"random_state\") or 125,\n",
        "        max_iter=config.get(\"max_iter\") or 100,\n",
        "    )\n",
        "\n",
        "    visualizer = KElbowVisualizer(model, k=(4,9), metric=\"silhouette\")\n",
        "    visualizer.fit(df_features)\n",
        "    visualizer.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "with the elbow method we now have a strong hint for the number of clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeanss = MiniBatchKMeans(\n",
        "    n_clusters=config.get(\"n_clusters\") or 6, \n",
        "    n_init=config.get(\"n_init\") or 10,\n",
        "    random_state=config.get(\"random_state\") or 125,\n",
        "    max_iter=config.get(\"max_iter\") or 100,\n",
        ")\n",
        "    \n",
        "\n",
        "\n",
        "# silhouette score visualisation\n",
        "visualizer = SilhouetteVisualizer(kmeanss, colors='yellowbrick')\n",
        "visualizer.fit(df_features)\n",
        "if config.get(\"save_model\"):\n",
        "    visualizer.show(outpath=f\"silhouette_score_clustering_{config.get('run_id')}.png\")\n",
        "else:\n",
        "    visualizer.show()\n",
        "    \n",
        "labels = kmeanss.fit_predict(df_features)\n",
        "df_features['Cluster'] = pd.Series(labels, index=df_features.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save model\n",
        "filename = \"sample_clustering_model\"  if config.get(\"sample\") else \"clustering_model\"\n",
        "filename = f\"{filename}_{config.get('n_clusters')}_{config.get('random_state')}\"\n",
        "\n",
        "\n",
        "if config.get(\"save_model\"):\n",
        "    with open(f\"{filename}_{config.get('run_id')}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(kmeanss, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if config.get(\"plot\"):\n",
        "    sns.countplot(x=df_features[\"Cluster\"]).set(title=\"clusters distribution\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimension reduction\n",
        "\n",
        "Since we have a lot of dimensions, we apply PCA in order to visualize the datapoints and clusters in a human interpretable way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# problem during PCA and kmeans pca\n",
        "# sklearn: Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these will not be consistent with labels_.\n",
        "# - 'nb_component' controls the variance of result - low value make obviously cleaner clusters\n",
        "# 1- compute eignvalues to know good pca 'nb_component'\n",
        "# 2- use generated value for pca\n",
        "\n",
        "# kmeans_pca.cluster_centers_ \n",
        "\n",
        "# PCA DEBUG\n",
        "# from sklearn.decomposition import PCA\n",
        "# import numpy as np\n",
        "\n",
        "# # Create an instance of PCA\n",
        "# pca = PCA()\n",
        "\n",
        "# # Fit the PCA model to your data\n",
        "# pca.fit(transactions_data)\n",
        "\n",
        "# # Get the explained variance ratio for each component\n",
        "# explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# # Select the number of components that retain 95% of the variance\n",
        "# n_components = np.argmax(np.cumsum(explained_variance) >= 0.95) + 1\n",
        "\n",
        "# # Set the n_components parameter of PCA to the selected number\n",
        "# pca = PCA(n_components=n_components)\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Create an instance of PCA\n",
        "# pca = PCA()\n",
        "\n",
        "# # Fit the PCA model to your data\n",
        "# pca.fit(transactions_data)\n",
        "\n",
        "# # Get the explained variance ratio for each component\n",
        "# explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# # Plot the explained variance ratio against the number of components\n",
        "# plt.plot(range(1, len(explained_variance)+1), explained_variance)\n",
        "# plt.xlabel('Number of components')\n",
        "# plt.ylabel('Explained variance ratio')\n",
        "# plt.show()\n",
        "\n",
        "# # Select the number of components where the explained variance ratio starts to decrease\n",
        "# n_components = int(input(\"Enter the number of components: \"))\n",
        "\n",
        "# # Set the n_components parameter of PCA to the selected number\n",
        "# pca = PCA(n_components=n_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "data_rescaled = scaler.fit_transform(df_features.drop(\"Cluster\", axis=1))\n",
        "reduced_data = PCA(n_components=config.get(\"n_components\") or 0.55).fit_transform(data_rescaled)\n",
        "kmeans_pca = MiniBatchKMeans(\n",
        "    n_clusters=config.get(\"n_clusters\") or 6, \n",
        "    n_init=config.get(\"n_init\") or 10,\n",
        "    random_state=config.get(\"random_state\") or 125,\n",
        "    max_iter=config.get(\"max_iter\") or 100,\n",
        ")\n",
        "labels_pca = kmeans_pca.fit_predict(reduced_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"sample_pca_clustering_model\"  if config.get(\"sample\") else \"pca_clustering_model\"\n",
        "filename = f\"{filename}_{config.get('n_clusters')}_{config.get('random_state')}_{config.get('run_id')}\"\n",
        "\n",
        "if config.get(\"save_model\"):\n",
        "    with open(f\"{filename}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(kmeans_pca, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "centroids = kmeans_pca.cluster_centers_\n",
        "u_labels = np.unique(labels_pca)\n",
        "  \n",
        "for i in u_labels:\n",
        "    plt.scatter(reduced_data[labels_pca == i , 0] , reduced_data[labels_pca == i , 1] , label = i)\n",
        "plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\"\"\n",
        "{kmeanss.inertia_}\n",
        "{kmeans_pca.inertia_}\n",
        "\"\"\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# analyser \n",
        " https://towardsdatascience.com/common-mistakes-in-cluster-analysis-and-how-to-avoid-them-eb960116d773   \n",
        " One fast and simple solution is to calculate the mean values of each feature per cluster (tbl. 2).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checkpoint\n",
        "# df_features.to_csv(\"../data/df_labelled.csv\")\n",
        "df_merged = df.merge(df_features.reset_index()[[\"Cluster\", \"CLI_ID\"]], on=\"CLI_ID\", how=\"left\")\n",
        "df_merged.to_csv(f\"../data/df_labelled_final_{config.get('run_id')}.csv\", index=False)\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = df_features.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "capilaire = df_test.groupby(\"Cluster\")['% CAPILLAIRES'].mean()\n",
        "hygiene = df_test.groupby(\"Cluster\")['% HYGIENE'].mean()\n",
        "maqu = df_test.groupby(\"Cluster\")['% MAQUILLAGE'].mean()\n",
        "parfum = df_test.groupby(\"Cluster\")['% PARFUMAGE'].mean()\n",
        "soins_corps = df_test.groupby(\"Cluster\")['% SOINS DU CORPS'].mean()\n",
        "soins_vis = df_test.groupby(\"Cluster\")['% SOINS DU VISAGE'].mean()\n",
        "solaire = df_test.groupby(\"Cluster\")['% SOLAIRES'].mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "edgecolor = \"red\"\n",
        "colors= ['black', 'red', 'green', 'blue', 'cyan', \"yellow\"]\n",
        "\n",
        "fig, axs = plt.subplots(4, 2,  figsize=(15, 15))\n",
        "fig.tight_layout(pad=3.0)\n",
        "\n",
        "\n",
        "axs[0,0].bar(capilaire.index, capilaire.values, color=colors, edgecolor=edgecolor)\n",
        "axs[0,0].set_title(\"mean value of % Capillaire by cluster\")\n",
        "axs[0,0].set(xlabel='cluster')\n",
        "\n",
        "axs[0,1].bar(hygiene.index, hygiene.values, color=colors, edgecolor=edgecolor)\n",
        "axs[0,1].set_title(\"mean value of % Hygiene by cluster\")\n",
        "axs[0,1].set(xlabel='cluster')\n",
        "\n",
        "axs[1,0].bar(maqu.index, maqu.values, color=colors, edgecolor=edgecolor)\n",
        "axs[1,0].set_title(\"mean value of % Maquillage by cluster\")\n",
        "axs[1,0].set(xlabel='cluster')\n",
        "\n",
        "axs[1,1].bar(parfum.index, parfum.values, color=colors, edgecolor=edgecolor)\n",
        "axs[1,1].set_title(\"mean value of % parfum by cluster\")\n",
        "axs[1,1].set(xlabel='cluster')\n",
        "\n",
        "axs[2,0].bar(soins_corps.index, soins_corps.values, color=colors, edgecolor=edgecolor)\n",
        "axs[2,0].set_title(\"mean value of % Soin corps by cluster\")\n",
        "axs[2,0].set(xlabel='cluster')\n",
        "\n",
        "axs[2,1].bar(soins_vis.index, soins_vis.values, color=colors, edgecolor=edgecolor)\n",
        "axs[2,1].set_title(\"mean value of % Soins visage by cluster\")\n",
        "axs[2,1].set(xlabel='cluster')\n",
        "\n",
        "axs[3,0].bar(solaire.index, solaire.values, color=colors, edgecolor=edgecolor)\n",
        "axs[3,0].set_title(\"mean value of % Solaire by cluster\")\n",
        "axs[3,0].set(xlabel='cluster')\n",
        "\n",
        "\n",
        "fig.delaxes(axs[3,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_sales = pd.concat([df_test, df.groupby(\"CLI_ID\")[\"PRIX_NET\"].mean()], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_by_cluster = total_sales.groupby(\"Cluster\")['PRIX_NET'].sum()\n",
        "mean_by_cluster = total_sales.groupby(\"Cluster\")['PRIX_NET'].mean()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, layout=\"constrained\")\n",
        "\n",
        "sns.barplot(ax=axes[0], x=total_by_cluster.index, y=total_by_cluster.values).set(title=\"Total sale by cluster\")\n",
        "sns.barplot(ax=axes[1] ,x=mean_by_cluster.index, y=mean_by_cluster.values).set(title=\"Mean sale by cluster\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "dataenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "79f75886e27c97162a7f267158f1cc4210dabbcbd350a56f8820e802547bda97"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
